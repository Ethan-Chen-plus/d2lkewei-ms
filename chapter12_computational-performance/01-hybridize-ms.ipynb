{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5073a8d",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 编译器和解释器\n",
    ":label:`sec_hybridize`\n",
    "\n",
    "目前为止，本书主要关注的是*命令式编程*（imperative programming）。\n",
    "命令式编程使用诸如`print`、“`+`”和`if`之类的语句来更改程序的状态。\n",
    "考虑下面这段简单的命令式程序：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35874531",
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "\n",
    "print(fancy_func(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036df1b",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "Python是一种*解释型语言*（interpreted language）。因此，当对上面的`fancy_func`函数求值时，它按顺序执行函数体的操作。也就是说，它将通过对`e = add(a, b)`求值，并将结果存储为变量`e`，从而更改程序的状态。接下来的两个语句`f = add(c, d)`和`g = add(e, f)`也将执行类似地操作，即执行加法计算并将结果存储为变量。 :numref:`fig_compute_graph`说明了数据流。\n",
    "\n",
    "![命令式编程中的数据流](../img/computegraph.svg)\n",
    ":label:`fig_compute_graph`\n",
    "\n",
    "尽管命令式编程很方便，但可能效率不高。一方面原因，Python会单独执行这三个函数的调用，而没有考虑`add`函数在`fancy_func`中被重复调用。如果在一个GPU（甚至多个GPU）上执行这些命令，那么Python解释器产生的开销可能会非常大。此外，它需要保存`e`和`f`的变量值，直到`fancy_func`中的所有语句都执行完毕。这是因为程序不知道在执行语句`e = add(a, b)`和`f = add(c, d)`之后，其他部分是否会使用变量`e`和`f`。\n",
    "\n",
    "## 符号式编程\n",
    "\n",
    "考虑另一种选择*符号式编程*（symbolic programming），即代码通常只在完全定义了过程之后才执行计算。这个策略被多个深度学习框架使用，包括Theano和TensorFlow（后者已经获得了命令式编程的扩展）。一般包括以下步骤：\n",
    "\n",
    "1. 定义计算流程；\n",
    "1. 将流程编译成可执行的程序；\n",
    "1. 给定输入，调用编译好的程序执行。\n",
    "\n",
    "这将允许进行大量的优化。首先，在大多数情况下，我们可以跳过Python解释器。从而消除因为多个更快的GPU与单个CPU上的单个Python线程搭配使用时产生的性能瓶颈。其次，编译器可以将上述代码优化和重写为`print((1 + 2) + (3 + 4))`甚至`print(10)`。因为编译器在将其转换为机器指令之前可以看到完整的代码，所以这种优化是可以实现的。例如，只要某个变量不再需要，编译器就可以释放内存（或者从不分配内存），或者将代码转换为一个完全等价的片段。下面，我们将通过模拟命令式编程来进一步了解符号式编程的概念。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e05cfc5",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf56f89",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "命令式（解释型）编程和符号式编程的区别如下：\n",
    "\n",
    "* 命令式编程更容易使用。在Python中，命令式编程的大部分代码都是简单易懂的。命令式编程也更容易调试，这是因为无论是获取和打印所有的中间变量值，或者使用Python的内置调试工具都更加简单；\n",
    "* 符号式编程运行效率更高，更易于移植。符号式编程更容易在编译期间优化代码，同时还能够将程序移植到与Python无关的格式中，从而允许程序在非Python环境中运行，避免了任何潜在的与Python解释器相关的性能问题。\n",
    "\n",
    "## 混合式编程\n",
    "\n",
    "历史上，大部分深度学习框架都在命令式编程与符号式编程之间进行选择。例如，Theano、TensorFlow（灵感来自前者）、Keras和CNTK采用了符号式编程。相反地，Chainer和PyTorch采取了命令式编程。在后来的版本更新中，TensorFlow2.0和Keras增加了命令式编程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892a8bf",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "如上所述，PyTorch是基于命令式编程并且使用动态计算图。为了能够利用符号式编程的可移植性和效率，开发人员思考能否将这两种编程模型的优点结合起来，于是就产生了torchscript。torchscript允许用户使用纯命令式编程进行开发和调试，同时能够将大多数程序转换为符号式程序，以便在需要产品级计算性能和部署时使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7a98d",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## `Sequential`的混合式编程\n",
    "\n",
    "要了解混合式编程的工作原理，最简单的方法是考虑具有多层的深层网络。按照惯例，Python解释器需要执行所有层的代码来生成一条指令，然后将该指令转发到CPU或GPU。对于单个的（快速的）计算设备，这不会导致任何重大问题。另一方面，如果我们使用先进的8-GPU服务器，比如AWS P3dn.24xlarge实例，Python将很难让所有的GPU都保持忙碌。在这里，瓶颈是单线程的Python解释器。让我们看看如何通过将`Sequential`替换为`HybridSequential`来解决代码中这个瓶颈。首先，我们定义一个简单的多层感知机。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3704c-c162-46ff-a199-85ed601ea721",
   "metadata": {},
   "source": [
    "参考 https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.jit.html?highlight=jit#mindspore.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dea2e2",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1, 2], dtype=Float32, value=\n",
       "[[-1.29457220e-01,  1.18005425e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import nn, ops\n",
    "from d2lkewei import mindspore as d2l\n",
    "\n",
    "\n",
    "# 生产网络的工厂模式\n",
    "def get_net(x):\n",
    "    net = nn.SequentialCell(nn.Dense(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(128, 2))\n",
    "    return net(x)\n",
    "\n",
    "x = ops.randn((1, 512))\n",
    "# net = get_net()\n",
    "get_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7765c3",
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "通过使用`torch.jit.script`函数来转换模型，我们就有能力编译和优化多层感知机中的计算，而模型的计算结果保持不变。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cdc2f78",
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(3966641,7f028e615740,python):2023-09-13-21:19:57.345.521 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_3966641/376148590.py]\n",
      "[ERROR] CORE(3966641,7f028e615740,python):2023-09-13-21:19:57.345.702 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_3966641/376148590.py]\n",
      "[ERROR] CORE(3966641,7f028e615740,python):2023-09-13-21:19:57.345.907 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_3966641/376148590.py]\n",
      "[ERROR] ANALYZER(3966641,7f028e615740,python):2023-09-13-21:19:57.345.915 [mindspore/ccsrc/pipeline/jit/static_analysis/async_eval_result.cc:69] HandleException] Exception happened, check the information as below.\n",
      "\n",
      "The function call stack (See file '/home/daiyuxin/ckw/d2lkewei-ms/chapter12_computational-performance/rank_0/om/analyze_fail.ir' for more details. Get instructions about `analyze_fail.ir` at https://www.mindspore.cn/search?inputValue=analyze_fail.ir):\n",
      "# 0 In file /tmp/ipykernel_3966641/376148590.py:8\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_parameters_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mjit(get_net)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/common/api.py:807\u001b[0m, in \u001b[0;36mjit.<locals>.wrap_mindspore.<locals>.staging_specialize\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pynative_parallel() \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m _PYNATIVE_PARALLEL_FUNC_NAME:\n\u001b[1;32m    806\u001b[0m     process_obj \u001b[38;5;241m=\u001b[39m hash_args\n\u001b[0;32m--> 807\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_MindsporeFunctionExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/common/api.py:106\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 106\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/common/api.py:526\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    525\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mclear_res()\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecompile_only\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/common/api.py:520\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m context\u001b[38;5;241m.\u001b[39mPYNATIVE_MODE:\n\u001b[1;32m    519\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_ms_function_compile_status(\u001b[38;5;28;01mTrue\u001b[39;00m, phase)\n\u001b[0;32m--> 520\u001b[0m     phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_ms_function_compile_status(\u001b[38;5;28;01mFalse\u001b[39;00m, phase)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/common/api.py:599\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.compile\u001b[0;34m(self, method_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_executor\u001b[38;5;241m.\u001b[39mset_jit_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_config_dict)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 599\u001b[0m     is_compile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, ms\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCell):\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/_extends/parse/parser.py:465\u001b[0m, in \u001b[0;36mcreate_instance\u001b[0;34m(cls_type, params)\u001b[0m\n\u001b[1;32m    463\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cls_type(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args:\n\u001b[0;32m--> 465\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mcls_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kwargs:\n\u001b[1;32m    467\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cls_type(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindspore/nn/layer/container.py:187\u001b[0m, in \u001b[0;36mSequentialCell.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_child_to_cell(\u001b[38;5;28mstr\u001b[39m(index), cell)\n\u001b[0;32m--> 187\u001b[0m         \u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters_name\u001b[49m(\u001b[38;5;28mstr\u001b[39m(index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dynamic_name\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cells\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_parameters_name'"
     ]
    }
   ],
   "source": [
    "net = ms.jit(get_net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f491d8a",
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "我们编写与之前相同的代码，再使用`torch.jit.script`简单地转换模型，当完成这些任务后，网络就将得到优化（我们将在下面对性能进行基准测试）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2a62b",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### 通过混合式编程加速\n",
    "\n",
    "为了证明通过编译获得了性能改进，我们比较了混合编程前后执行`net(x)`所需的时间。让我们先定义一个度量时间的类，它在本章中在衡量（和改进）模型性能时将非常有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9f3e70",
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class Benchmark:\n",
    "    \"\"\"用于测量运行时间\"\"\"\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = d2l.Timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551f613",
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "现在我们可以调用网络两次，一次使用torchscript，一次不使用torchscript。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf71215",
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无torchscript: 0.6886 sec\n",
      "有torchscript: 0.2695 sec\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "with Benchmark('无torchscript'):\n",
    "    for i in range(1000): net(x)\n",
    "\n",
    "net = torch.jit.script(net)\n",
    "with Benchmark('有torchscript'):\n",
    "    for i in range(1000): net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53e1a4",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "如以上结果所示，在`nn.Sequential`的实例被函数`torch.jit.script`脚本化后，通过使用符号式编程提高了计算性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148db16",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "### 序列化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053182a",
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "编译模型的好处之一是我们可以将模型及其参数序列化（保存）到磁盘。这允许这些训练好的模型部署到其他设备上，并且还能方便地使用其他前端编程语言。同时，通常编译模型的代码执行速度也比命令式编程更快。让我们看看`save`的实际功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ec5182",
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 652K Jul  9 16:11 my_mlp\n"
     ]
    }
   ],
   "source": [
    "net.save('my_mlp')\n",
    "!ls -lh my_mlp*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d661199",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 命令式编程使得新模型的设计变得容易，因为可以依据控制流编写代码，并拥有相对成熟的Python软件生态。\n",
    "* 符号式编程要求我们先定义并且编译程序，然后再执行程序，其好处是提高了计算性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e376ed",
   "metadata": {
    "origin_pos": 62
   },
   "source": [
    "## 练习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e806161",
   "metadata": {
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. 回顾前几章中感兴趣的模型，能提高它们的计算性能吗？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340c531",
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2788)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

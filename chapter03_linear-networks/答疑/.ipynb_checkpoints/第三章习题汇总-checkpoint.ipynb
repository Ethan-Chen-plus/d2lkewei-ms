{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503b2193-701d-4b56-ab1f-10c0a24a31d7",
   "metadata": {},
   "source": [
    "# 第三章习题解答汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d300b-17a9-459a-8f8c-b96179323138",
   "metadata": {},
   "source": [
    "by Kewei Chen\n",
    "- kewei_chen@foxmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb62ad8-1f8b-45b8-931f-4430a097b7b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [linear-regression](linear-regression.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db7948-22ef-49e1-9f72-f7c27d44f55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 练习\n",
    "\n",
    "1. 假设我们有一些数据$x_1, \\ldots, x_n \\in \\mathbb{R}$。我们的目标是找到一个常数$b$，使得最小化$\\sum_i (x_i - b)^2$。\n",
    "    1. 找到最优值$b$的解析解。\n",
    "    1. 这个问题及其解与正态分布有什么关系?\n",
    "1. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置$b$（我们可以通过向$\\mathbf X$添加所有值为1的一列来做到这一点）。\n",
    "    1. 用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。\n",
    "    1. 计算损失对$w$的梯度。\n",
    "    1. 通过将梯度设为0、求解矩阵方程来找到解析解。\n",
    "    1. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？\n",
    "1. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "    1. 写出模型$-\\log P(\\mathbf y \\mid \\mathbf X)$下数据的负对数似然。\n",
    "    1. 请试着写出解析解。\n",
    "    1. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad1620-86ef-40e9-91d1-2950cd83289a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516b931-748b-40c8-8aff-fe59f4320bde",
   "metadata": {},
   "source": [
    "1. 假设我们有一些数据$x_1, \\ldots, x_n \\in \\mathbb{R}$。我们的目标是找到一个常数$b$，使得最小化$\\sum_i (x_i - b)^2$。\n",
    "    1. 找到最优值$b$的解析解。\n",
    "    1. 这个问题及其解与正态分布有什么关系?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add8343-56f8-445a-aabd-c271265bba33",
   "metadata": {},
   "source": [
    "1. 解答：\n",
    "    1. $b$的最优解是$x_1, \\ldots, x_n$的平均值。这是因为$\\sum_i (x_i - b)^2 = \\sum_i (x_i - \\bar{x})^2 + n(\\bar{x} - b)^2$，其中$\\bar{x}$是$x_1, \\ldots, x_n$的平均值。由于$n(\\bar{x} - b)^2 \\geq 0$，因此当且仅当$b = \\bar{x}$时，$\\sum_i (x_i - b)^2$最小。\n",
    "    2. 这个问题与正态分布有关，因为如果我们假设$x_1, \\ldots, x_n$是从正态分布中采样得到的，那么$b = \\bar{x}$是最大似然估计下的均值。也就是说，如果我们假设数据来自正态分布，并且我们想要找到一个最好的均值来描述这些数据，那么我们应该选择样本均值作为估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a67c0-8b20-4ffa-bada-46411f62ccb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cef53-d6a7-4647-8d6c-1ec99efac9e7",
   "metadata": {},
   "source": [
    "2. 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置$b$（我们可以通过向$\\mathbf X$添加所有值为1的一列来做到这一点）。\n",
    "    1. 用矩阵和向量表示法写出优化问题（将所有数据视为单个矩阵，将所有目标值视为单个向量）。\n",
    "    1. 计算损失对$w$的梯度。\n",
    "    1. 通过将梯度设为0、求解矩阵方程来找到解析解。\n",
    "    1. 什么时候可能比使用随机梯度下降更好？这种方法何时会失效？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861fce3-851a-4035-8003-63f69c2efdb8",
   "metadata": {},
   "source": [
    "2. 解答：\n",
    "    1. 设$\\mathbf X \\in \\mathbb R^{n \\times d}$是输入特征，$\\mathbf y \\in \\mathbb R^n$是标签。线性回归的目标是找到一组权重向量$\\mathbf w \\in \\mathbb R^d$，使得预测值$\\hat{\\mathbf y} = \\mathbf X\\mathbf w$与真实标签$\\mathbf y$之间的平方误差最小化。忽略偏置$b$，优化问题可以写成以下形式：\n",
    "    $$\n",
    "    \\min_{\\mathbf w} \\frac{1}{2n} \\| \\mathbf X\\mathbf w - \\mathbf y \\|_2^2.\n",
    "    $$\n",
    "\n",
    "    2. 损失函数对权重向量$\\mathbf w$的梯度为\n",
    "\n",
    "    $$\n",
    "    \\nabla_{\\mathbf w} L(\\mathbf w) = \\frac{1}{n} \\mathbf X^\\top (\\hat{\\mathbf y} - \\mathbf y).\n",
    "    $$\n",
    "\n",
    "    3. 将梯度设为0，我们得到了解析解：\n",
    "\n",
    "    $$\n",
    "    \\hat{\\mathbf w} = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y.\n",
    "    $$\n",
    "\n",
    "    4. 当数据集较小时，解析解可能比随机梯度下降更好。然而，在大型数据集上，计算解析解可能会非常耗时，或者存在多个局部最小的情况。此外，当矩阵$\\mathbf X^\\top\\mathbf X$不可逆时，解析解不存在。在这种情况下，我们需要使用正则化或数值优化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3af88-bb23-4f53-a39f-5f316cccb8ef",
   "metadata": {},
   "source": [
    "第二题详细步骤：\n",
    "对于线性回归问题，损失函数是平方误差。假设我们有一个数据集$\\{(\\mathbf x_1, y_1), \\ldots, (\\mathbf x_n, y_n)\\}$，其中$\\mathbf x_i \\in \\mathbb R^d$是输入特征，$y_i \\in \\mathbb R$是标签。我们的目标是找到一组权重向量$\\mathbf w \\in \\mathbb R^d$，使得预测值$\\hat{y}_i = \\mathbf w^\\top\\mathbf x_i$与真实标签$y_i$之间的平方误差最小化。因此，损失函数可以写成以下形式：\n",
    "\n",
    "$$\n",
    "L(\\mathbf w) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\| \\mathbf X\\mathbf w - \\mathbf y \\|_2^2,\n",
    "$$\n",
    "\n",
    "其中$\\mathbf X = [\\mathbf x_1^\\top, \\ldots, \\mathbf x_n^\\top]^\\top$是输入特征的矩阵，$\\mathbf y = [y_1, \\ldots, y_n]^\\top$是标签向量。\n",
    "\n",
    "损失函数对权重向量$\\mathbf w$的梯度为\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf w} L(\\mathbf w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\nabla_{\\mathbf w} \\hat{y}_i = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\mathbf x_i,\n",
    "$$\n",
    "\n",
    "其中$\\hat{\\mathbf y} = [\\hat{y}_1, \\ldots, \\hat{y}_n]^\\top$是预测值向量。将其写成矩阵和向量表示法，我们得到：\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf w} L(\\mathbf w) =  \\frac{1}{n}  (\\hat{\\mathbf y} -  {\\mathbf y})^\\top {\\mathbf X}.\n",
    "$$\n",
    "\n",
    "因此，\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf w} L(\\mathbf w) =  \\frac{1}{n}  {\\mathbf X}^\\top (\\hat{\\mathbf y} -  {\\mathbf y}),\n",
    "$$\n",
    "\n",
    "\n",
    "我们要求解的是：$$ \\nabla_{\\mathbf w} L(\\mathbf w) = \\frac{1}{n} \\mathbf X^\\top (\\hat{\\mathbf y} - \\mathbf y) = 0 $$\n",
    "\n",
    "其中，$\\hat{\\mathbf y}$ 是预测值，$\\mathbf y$ 是真实值，$\\mathbf X$ 是输入数据。我们可以将上式变形为：\n",
    "\n",
    "$$ \\mathbf X^\\top (\\hat{\\mathbf y} - \\mathbf y) = 0 $$\n",
    "\n",
    "进一步变形得到：\n",
    "\n",
    "$$ \\mathbf X^\\top \\hat{\\mathbf y} = \\mathbf X^\\top \\mathbf y $$\n",
    "\n",
    "因为 $\\hat{\\mathbf y} = \\mathbf X\\hat{\\mathbf w}$，所以：\n",
    "\n",
    "$$ \\begin{aligned} &\\quad\\; \\mathbf X^\\top (\\hat{\\mathbf y} - \\mathbf y) = 0 \\\\ &\\Rightarrow\\; \\mathbf X^\\top ({\\mathbf X}{\\mathbf w} - \\mathbf y) = 0 \\\\ &\\Rightarrow\\; {\\mathbf w} = ({\\mathbf X}^\\top{\\mathbf X})^{-1}{\\mathbf X}^\\top\\mathbf y \\\\ \\end{aligned} $$\n",
    "\n",
    "这就是推导过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccdcde-3ca8-488b-9866-a509a1bddb96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第三题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7e60d-7b54-4812-8eb7-15416c3c3bb5",
   "metadata": {},
   "source": [
    "3. 假定控制附加噪声$\\epsilon$的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "    1. 写出模型$-\\log P(\\mathbf y \\mid \\mathbf X)$下数据的负对数似然。\n",
    "    1. 请试着写出解析解。\n",
    "    1. 提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69c0c6-b210-470b-9acd-997e73e7cc71",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "3. 解答\n",
    "    1. \n",
    "    对于高斯分布而言，其概率密度函数为：$$p(y) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}$$ 其中μ是均值，σ是标准差。\n",
    "\n",
    "    将上式取负对数并展开得到：$$-\\log p(y) = \\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{(y-\\mu)^2}{2\\sigma^2} $$\n",
    "\n",
    "    对于线性回归模型而言，它的概率密度函数为：$$p(y|x,w,b) = N(y|Xw+b,\\sigma^2)$$ 其中N表示高斯分布。\n",
    "\n",
    "    因此，我们可以得到负对数似然函数的结果为：$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\log p(y_i|x_i,w,b) = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}||y-Xw-b||^2_2 $$\n",
    "\n",
    "    其中C是一个常数，n是样本数量。\n",
    "    \n",
    "    因此，我们可以得到负对数似然函数的结果为：$$-\\log P(\\mathbf y \\mid \\mathbf X) = C + n\\log 2 + \\sum_{i=1}^n |\\epsilon_i|$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275fac5-83cf-4362-8151-1c411af5d075",
   "metadata": {},
   "source": [
    "提出一种随机梯度下降算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）请尝试解决这个问题。\n",
    "\n",
    "- 随机梯度下降算法（SGD）是一种用于优化目标函数的迭代方法。在每次迭代中，SGD从训练集中随机选择一个样本$(\\mathbf x_i, y_i)$并计算其梯度。然后它使用该梯度来更新参数：\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  &\\mathbf w^{(t+1)} = \\mathbf w^{(t)} - \\eta_t\\nabla_{\\mathbf w} L(y_i, f(\\mathbf x_i; \\theta)), \\\\\n",
    "  &b^{(t+1)} = b^{(t)} - \\eta_t\\nabla_b L(y_i, f(\\mathbf x_i; \\theta)),\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "  其中$L(y,f(\\cdot;\\theta))$是损失函数，$\\eta_t$是学习率，$\\theta=(\\mathbf w,b)$是模型参数。\n",
    "\n",
    "  在本例中，我们可以使用均方误差作为损失函数：\n",
    "  $$\n",
    "  L(y,f(\\cdot;\\theta)) = (y - (\\mathbf w^\\top\\mathbf x + b))^2.\n",
    "  $$\n",
    "\n",
    "  然而，在驻点附近可能会发生以下情况：当梯度接近零时，步长也会变得非常小，这可能导致算法收敛速度变慢。为了解决这个问题，我们可以使用一些技巧来加速收敛速度。例如，我们可以使用动量或自适应学习率等技术来加速收敛速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a983c-a53c-4307-bd41-51337081ed82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a5fccbb-c0f9-4191-bc8b-d4da74450157",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [linear-regression-scratch](linear-regression-scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e9db7-f6d6-4794-ba25-0daa67078907",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "origin_pos": 40,
    "tags": []
   },
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？\n",
    "1. 假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?\n",
    "1. 能基于[普朗克定律](https://en.wikipedia.org/wiki/Planck%27s_law)使用光谱能量密度来确定物体的温度吗？\n",
    "1. 计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？\n",
    "1. 为什么在`squared_loss`函数中需要使用`reshape`函数？\n",
    "1. 尝试使用不同的学习率，观察损失函数值下降的快慢。\n",
    "1. 如果样本个数不能被批量大小整除，`data_iter`函数的行为会有什么变化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21879249-855f-44a3-bd35-2624a956844d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f467b-bba1-493f-8587-689caee1d74d",
   "metadata": {},
   "source": [
    "1. 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679de0f-3c93-4b90-bc2e-0ffe8f72e7e9",
   "metadata": {},
   "source": [
    "1. 如果将权重初始化为零，那么每个神经元的输出都是相同的，这意味着每个神经元学习到的参数也是相同的。因此，每个神经元都会更新相同的参数，最终导致所有神经元学习到相同的特征。因此，权重初始化为零会使算法失效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99cd5f-9169-48e5-9ef2-5ddc32589e74",
   "metadata": {},
   "source": [
    "注意，把w,b全部初始化为0是不会影响的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3508e-2f61-4638-a1ef-d7e4f84a2b46",
   "metadata": {},
   "source": [
    "权重初始化为零会使算法失效的说法是正确的。如果将权重初始化为零，那么每个神经元的输出都是相同的，这意味着每个神经元学习到的参数也是相同的。因此，每个神经元都会更新相同的参数，最终导致所有神经元学习到相同的特征。这样就失去了神经网络的优势，即可以学习到不同特征的能力。\n",
    "\n",
    "如果在使用全零初始化时得到了最终结果，可能是因为使用了其他技巧来避免这种情况发生。例如，在训练过程中使用了正则化或者dropout等技术，这些技术可以帮助避免所有神经元学习到相同的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d284dab-bae6-44d0-bb92-219297374548",
   "metadata": {},
   "source": [
    "逻辑回归可以存在权重初始化为0，这个说法的具体解释可以参考如下链接：\n",
    "https://zhuanlan.zhihu.com/p/75879624"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a1f3-5a1e-420f-af54-8b6631cd179a",
   "metadata": {},
   "source": [
    "逻辑回归和神经网络有不同的权重初始化方法。对于逻辑回归，可以将权重初始化为零，因为这是一个线性模型，梯度下降算法仍然可以更新它们。然而，对于神经网络来说，将权重初始化为零可能会导致对称性问题，并阻止隐藏单元学习不同的特征。因此，最好使用随机或其他方法来初始化神经网络的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba616e1-55b1-4e2d-84d6-c30d31cb25f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271f2f5-42cd-4ef5-ba2b-aec4104ed992",
   "metadata": {},
   "source": [
    "2. 假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea415f6a-8bd7-4902-a71d-cd06f4edf8d5",
   "metadata": {},
   "source": [
    "自动微分（Automatic Differentiation，简称AD）是一种对计算机程序进行高效准确求导的技术。它是介于符号微分和数值微分之间的一种方法，可以计算可导函数在某点处的导数值的计算，是反向传播算法的一般化。\n",
    "\n",
    "自动微分要解决的核心问题是计算复杂函数，通常是多层复合函数在某一点处的导数、梯度以及Hessian矩阵值\n",
    "torch中的backward就是自动微分。backward()函数会自动计算所有需要求导的变量的梯度，并将结果存储在相应变量的grad属性中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22aa2a4b-76a7-4f85-9c95-73075e596c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[2.9084]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0168], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 生成数据\n",
    "x = torch.randn(100, 1)\n",
    "y = 3 * x + 0.5 * torch.randn(100, 1)\n",
    "\n",
    "# 定义模型\n",
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(1000):\n",
    "    # 前向传播\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471a239-77d6-4c04-9eee-1edc83d3c006",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2399f5-144b-4d0d-89e8-08413e46e049",
   "metadata": {},
   "source": [
    "3. 能基于[普朗克定律](https://en.wikipedia.org/wiki/Planck%27s_law)使用光谱能量密度来确定物体的温度吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b67eae-57a7-4bfc-a64f-06706648b21a",
   "metadata": {},
   "source": [
    "3. 是的，可以使用普朗克定律来确定物体的温度。普朗克定律描述了黑体辐射的能量密度与温度之间的关系。通过测量物体发出的辐射能量密度，并使用普朗克定律，我们可以确定物体的温度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224777bf-a7e1-4120-b3e8-1bcfe29a0751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第4题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d087a-c4f6-4cda-82be-85d695accde5",
   "metadata": {},
   "source": [
    "4. 计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c77b8f-42bf-4e4a-8248-64bcc118a44f",
   "metadata": {},
   "source": [
    "4. 在计算二阶导数时可能会遇到数值不稳定性问题。这些问题可以通过使用更高精度的数据类型（例如双精度浮点数）或通过使用数值稳定性技巧（例如中心差分）来解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddd539-80a0-4bcf-981f-6e80b94ab22c",
   "metadata": {},
   "source": [
    "数值不稳定性是指在数值计算过程中，由于舍入误差、截断误差等原因，导致计算结果的精度出现大幅度波动或者发散。例如，在计算二阶导数时，如果使用简单的有限差分公式，可能会出现数值不稳定性问题。这些问题可以通过使用更高精度的数据类型（例如双精度浮点数）或通过使用数值稳定性技巧（例如中心差分）来解决。\n",
    "\n",
    "中心差分是一种常用的数值稳定性技巧，它可以用于计算函数在某个点处的导数。具体来说，中心差分可以通过以下公式计算：\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}$$\n",
    "\n",
    "其中 $h$ 是一个很小的正数，通常取 $10^{-6}$ 或更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e6c24-501b-4ec3-8538-c86fb489bc1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第5题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78022a-274e-4093-ab6c-ef04286da845",
   "metadata": {},
   "source": [
    "5. 为什么在`squared_loss`函数中需要使用`reshape`函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d5f41-e693-453d-b1ca-1176b6cde09e",
   "metadata": {},
   "source": [
    "请仔细阅读书上的代码，这里使用reshape是为了保证y和y_hat形状相同，避免触发广播机制导致错误的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e611bb-c4d8-4a4b-a519-ddd0e3da9003",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第6题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d1341-8c97-4c2b-9f7d-46b1bc23080a",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. 尝试使用不同的学习率，观察损失函数值下降的快慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372452c-4297-498a-8567-7bb7e3166a82",
   "metadata": {},
   "source": [
    "```py\n",
    "lr=[0.1,0.001,0.001,0.05,..]\n",
    "for lr:\n",
    "    model.train\n",
    "```\n",
    "学习率有一个最佳的取值，可以不断尝试，既不能太大，也不能太小，同时不同的模型最佳学习率也是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294c1b2-37cf-4e76-91d7-4e6e3da3ee3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 第7题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa62bd-a77f-4791-9cc1-8551c5b356d0",
   "metadata": {},
   "source": [
    "7. 如果样本个数不能被批量大小整除，`data_iter`函数的行为会有什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46873774-a6d4-4b31-9f4f-d549caa3e696",
   "metadata": {},
   "source": [
    "如果样本个数不能被批量大小整除，则在最后一个迭代周期中，最后一批次可能包含少于批量大小个样本。在这种情况下，我们只需忽略该批次中多余的样本即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cad73-f8cf-4f7e-9618-480f608b4970",
   "metadata": {},
   "source": [
    "例如，1000个总样本，batch_size=3,那么最后1个样本会被舍去"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1321d-4d1e-4ef6-9926-a9744745f42b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [linear-regression-concise](linear-regression-concise.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef1352-8a52-432b-b155-c884112fec06",
   "metadata": {
    "origin_pos": 67
   },
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？\n",
    "1. 查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失，即\n",
    "    $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\text{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\text{ 其它情况}\\end{cases}$$\n",
    "1. 如何访问线性回归的梯度？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30c7b3-4065-4e82-8f3c-6a954b38c927",
   "metadata": {},
   "source": [
    "1. 如果将小批量的总损失替换为小批量损失的平均值，则需要将学习率乘以批量大小。这是因为在计算梯度时，我们使用了小批量中所有样本的信息。因此，如果我们将小批量的总损失替换为小批量损失的平均值，则相当于将每个样本的梯度除以批量大小。因此，我们需要将学习率乘以批量大小，以保持相同的更新步长。\n",
    "\n",
    "2. 不同的深度学习框架提供了不同的损失函数和初始化方法。例如，在PyTorch中，可以使用`torch.nn.MSELoss`类来计算均方误差损失，并使用`torch.nn.init.normal_`函数来初始化模型参数。要使用Huber损失代替原损失，可以自定义一个新的损失函数，并在训练过程中使用它。\n",
    "\n",
    "3. 要访问线性回归模型的梯度，可以使用自动微分技术。在PyTorch中，可以通过调用`backward()`方法来计算模型参数相对于损失函数的梯度。然后，可以通过访问模型参数的`.grad`属性来获取梯度值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cf0a1-200c-4651-bb7c-403919f6bb45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [softmax-regression](softmax-regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9072879-82a6-4a2c-b712-c1495d99454b",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 我们可以更深入地探讨指数族与softmax之间的联系。\n",
    "    1. 计算softmax交叉熵损失$l(\\mathbf{y},\\hat{\\mathbf{y}})$的二阶导数。\n",
    "    1. 计算$\\mathrm{softmax}(\\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配。\n",
    "1. 假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "    1. 如果我们尝试为它设计二进制代码，有什么问题？\n",
    "    1. 请设计一个更好的代码。提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码$n$个观测值怎么办？\n",
    "1. softmax是对上面介绍的映射的误称（虽然深度学习领域中很多人都使用这个名字）。真正的softmax被定义为$\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$。\n",
    "    1. 证明$\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$。\n",
    "    1. 证明$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$成立，前提是$\\lambda > 0$。\n",
    "    1. 证明对于$\\lambda \\to \\infty$，有$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$。\n",
    "    1. soft-min会是什么样子？\n",
    "    1. 将其扩展到两个以上的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daf2bb-bf59-4fac-b327-aac8912800cc",
   "metadata": {},
   "source": [
    "## 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fe1c2-b397-44f2-868c-6ce97dfd3c9b",
   "metadata": {},
   "source": [
    "1. 我们可以更深入地探讨指数族与softmax之间的联系。\n",
    "    1. 计算softmax交叉熵损失$l(\\mathbf{y},\\hat{\\mathbf{y}})$的二阶导数。\n",
    "    1. 计算$\\mathrm{softmax}(\\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c4f9b-57d6-4bf2-8e7a-7f314fc2f71c",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "考虑相对于任何未规范化的预测$o_j$的导数，我们得到：\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e2e87-4215-4bd1-a8e8-35848ff84d92",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_{o_j}^2 l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)*\\sum_{k=1}^q \\exp(o_k)-\\exp(o_j)^2}{(\\sum_{k=1}^q \\exp(o_k))^2}\\\\\n",
    "= \\mathrm{softmax}(\\mathbf{o})_j - (\\mathrm{softmax}(\\mathbf{o})_j)^2\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25be78-7041-4d96-b5e6-4d5d539b8f95",
   "metadata": {},
   "source": [
    "对于softmax交叉熵损失函数$l(\\mathbf{y},\\hat{\\mathbf{y}})$，其二阶导数为：\n",
    "\n",
    "$$ \\begin{aligned} \\partial_{o_j}^2 l(\\mathbf{y}, \\hat{\\mathbf{y}}) &= \\mathrm{softmax}(\\mathbf{o})_j - (\\mathrm{softmax}(\\mathbf{o})_j)^2\\\\ &= \\mathrm{softmax}(\\mathbf{o})_j(1-\\mathrm{softmax}(\\mathbf{o})_j).\\\\ \\end{aligned} $$\n",
    "\n",
    "其中，$\\mathrm{softmax}(\\mathbf{o})$是由向量$\\mathbf{o}$的元素通过softmax函数计算得到的概率分布。\n",
    "\n",
    "对于softmax函数$\\mathrm{softmax}(\\mathbf{o})$，其分布方差为：\n",
    "\n",
    "$$ \\begin{aligned} \\mathrm{Var}_{\\mathrm{softmax}(\\mathbf{o})} &= \\sum_{j=1}^q (\\mathrm{softmax}(\\mathbf{o})_j - E[\\mathrm{softmax}(\\mathbf{o})_j])^2\\\\ &= \\sum_{j=1}^q (\\mathrm{softmax}(\\mathbf{o})_j - \\frac{1}{q}\\sum_{k=1}^q \\mathrm{softmax}(\\mathbf{o})_k)^2\\\\ &= \\sum_{j=1}^q (\\mathrm{softmax}(\\mathbf{o})_j - \\frac{1}{q})^2.\\\\ \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a8ab5-e4bb-4b58-848d-2c4611a33c53",
   "metadata": {},
   "source": [
    "## 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae445d86-d58b-40f4-a1c3-3e4953dd3f55",
   "metadata": {},
   "source": [
    "2. 假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "    1. 如果我们尝试为它设计二进制代码，有什么问题？\n",
    "    1. 请设计一个更好的代码。提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码$n$个观测值怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc79b3-892b-41f2-a699-7a0672608f41",
   "metadata": {},
   "source": [
    "2. 假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "    1. 如果我们尝试为它设计二进制代码，会出现问题。因为如果我们使用两个独立的观察结果进行编码，则需要至少两个比特才能区分三个类别。但是，这意味着我们的平均长度为$\\frac{2}{3}$比特，而不是最优长度$\\log_2 3 \\approx 1.585$比特。\n",
    "    1. 我们可以使用联合编码来解决这个问题。具体来说，我们可以将$n$个观测值视为一个$n$元组，并将其映射到一个整数。例如，如果$n=2$，则可以将$(0, 0)$映射到0，$(0, 1)$映射到1，$(1, 0)$映射到2和$(1, 1)$映射到3。这样做的好处是我们可以使用$\\lceil \\log_2 3 \\rceil = 2$比特来编码三个类别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e78ec-e671-4e4b-9134-1327ebee3bb4",
   "metadata": {},
   "source": [
    "联合编码是一种编码方法，其中我们使用长度为$k=\\lceil \\log_2 {n+2 \\choose 2} \\rceil$的二进制代码来表示$n$个观测值的联合分布。这种方法可以用于任何概率分布，并且在实践中通常比独立编码更好。\n",
    "\n",
    "相比之下，二进制编码是一种特殊情况，其中我们尝试使用长度为$k$的二进制代码来表示一个概率向量。这种方法只适用于某些特殊的概率分布，并且在实践中可能会有问题。\n",
    "\n",
    "请注意，这些编码方法只是信息论中的两个例子，并且还有许多其他编码方法可供选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f7533-d8d4-48a1-bdb5-ff19532f6f38",
   "metadata": {},
   "source": [
    "## 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3efeba-0b1b-4695-8986-7c7ffbfdc887",
   "metadata": {},
   "source": [
    "\n",
    "1. $\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b)) > \\log (\\exp(a)) = a$且$\\mathrm{RealSoftMax}(a, b) > b$，因此$\\mathrm{RealSoftMax}(a, b) > \\max(a,b)$。\n",
    "2. $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) = \\lambda^{-1} \\log (\\exp(\\lambda a) + \\exp(\\lambda b)) > \\lambda^{-1} \\log (\\max(\\exp(\\lambda a),\\exp(\\lambda b))) = \\max(a,b)$，因此$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\max(a,b)$成立，前提是$\\lambda > 0$。\n",
    "3. $\\lim_{\\lambda\\to\\infty}\\mathrm{RealSoftMax}(\\lambda a,\\lambda b)=\\lim_{\\lambda\\to\\infty}\\log(e^{\\lambda a}+e^{\\lambda b})=\\lim_{\\lambda\\to\\infty}\\log(e^{\\max(a,b)}(e^{|\\min(a,b)-\\max(a,b)|}))=\\max(a,b)$，因此对于$\\lambda\\to\\infty$，有$\\lambda^{-1}\\mathrm{RealSoftMax}(\\lambda a,\\lambda b)\\to\\max(a,b)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b0db2-fda5-4513-942e-3a3be5bae90e",
   "metadata": {},
   "source": [
    "Softmin函数是softmax函数的变体，它将输入张量的每个元素$x_i$替换为$\\exp(-x_i)$，然后对结果进行归一化。Softmin函数的公式如下：\n",
    "$$\\mathrm{Softmin}(x_i) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}.$$\n",
    "与softmax函数类似，Softmin函数也可以用于多分类问题。不同之处在于，当输入张量中的元素越大时，Softmax函数会使输出概率越大，而Softmin函数则会使输出概率越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac966327-a94a-4c41-9df9-36846e5e8a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [image-classification-dataset](image-classification-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199cb93-eb64-4bf2-b585-596341d14495",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 减少`batch_size`（如减少到1）是否会影响读取性能？\n",
    "1. 数据迭代器的性能非常重要。当前的实现足够快吗？探索各种选择来改进它。\n",
    "1. 查阅框架的在线API文档。还有哪些其他数据集可用？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df93f0a-59dd-4e49-b492-8a9c557aadf8",
   "metadata": {},
   "source": [
    "1. 减少`batch_size`可能会影响读取性能。具体来说，当`batch_size`减小时，每个小批量的处理时间将增加，从而导致读取性能下降。此外，较小的批量大小可能会导致内存使用率更高。\n",
    "\n",
    "2. 数据迭代器的性能对于训练深度学习模型非常重要。当前的实现可能足够快，但是我们可以探索各种选择来改进它。例如，我们可以使用多线程或异步数据读取来加速数据迭代器。此外，我们还可以使用GPU加速来加速数据预处理和增强。\n",
    "\n",
    "3. 深度学习框架通常提供多个标准数据集，例如MNIST、CIFAR-10和ImageNet等。此外，还有许多其他数据集可用于特定领域的任务。您可以查阅框架的在线API文档以获取更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56710884-47cc-4500-a4db-075c41d0d262",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [softmax-regression-scratch](softmax-regression-scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f453085-afcc-4200-a17e-2bc9a8249c3b",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 本节直接实现了基于数学定义softmax运算的`softmax`函数。这可能会导致什么问题？提示：尝试计算$\\exp(50)$的大小。\n",
    "1. 本节中的函数`cross_entropy`是根据交叉熵损失函数的定义实现的。它可能有什么问题？提示：考虑对数的定义域。\n",
    "1. 请想一个解决方案来解决上述两个问题。\n",
    "1. 返回概率最大的分类标签总是最优解吗？例如，医疗诊断场景下可以这样做吗？\n",
    "1. 假设我们使用softmax回归来预测下一个单词，可选取的单词数目过多可能会带来哪些问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba21f75-182d-4574-8e54-6b9db02fd337",
   "metadata": {},
   "source": [
    "### 补充知识：prompt分类\n",
    "今天的天气很[MASK]\n",
    "好，坏，差，行\n",
    "相当于softmax预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266d8f64-f5b5-4921-a2af-d2ee2df3b89a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.184705528587072e+21"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "exp(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1942158c-1463-463c-912c-9db9b6c57498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.184705528587072e+21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf0bb7-bdd8-4a68-ab64-7de091053be8",
   "metadata": {},
   "source": [
    "1. 由于指数函数的值域是$(0,\\infty)$，因此可能会出现数值上溢的问题。这就是说，由于$\\exp(50)$的结果非常大，它可能超出计算机所能表示的范围，从而被近似为无穷大（inf）。这会带来一些问题，例如在反向传播时可能会出现NaN（不是数字）的情况。解决这个问题的一种常用技巧是，在计算softmax之前，先从所有输入中减去输入中的最大值。这样可以确保指数函数的输入不会太大而导致数值上溢。\n",
    "\n",
    "2. 交叉熵损失函数定义了$log$函数。当模型预测概率为0时，$log$函数的值为负无穷。因此，在实践中，我们通常忽略预测概率接近0的样本对损失函数的贡献。这可能会导致模型过度自信，并且在训练期间难以收敛。\n",
    "\n",
    "3. 解决第一个问题的方法是使用稳定版本的softmax运算。具体来说，我们可以先通过减去输入中的最大值来缩放softmax运算的输出。解决第二个问题的方法是使用交叉熵损失函数的平滑版本，例如标签平滑或温和交叉熵。\n",
    "\n",
    "4. 在某些情况下，返回概率最大的分类标签可能不是最优解。例如，在医疗诊断场景下，我们更关心误诊率和漏诊率等错误类型之间的权衡。在这种情况下，我们需要考虑其他评估指标，并根据特定应用程序选择合适的阈值。\n",
    "\n",
    "5. 如果可选取单词数量过多，则需要计算更多参数并增加模型复杂度。此外，在训练期间需要处理更多数据，并且预测时间也会变得更长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72d90f1d-53d8-4633-ab5b-da7d4792b74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., nan, 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第三题：\n",
    "x=torch.Tensor([[9,100,1]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "332bb8e8-8246-4972-863b-0669a34d4151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0144e-40, 1.0000e+00, 1.0089e-43]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第三题：\n",
    "x=torch.Tensor([[9-100,100-100,1-100]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cc732-1224-4194-9ffc-cd35306bce8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# - [softmax-regression-concise](softmax-regression-concise.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfdeb2-cb2c-483d-8d63-67bb35e8ad2c",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 尝试调整超参数，例如批量大小、迭代周期数和学习率，并查看结果。\n",
    "1. 增加迭代周期的数量。为什么测试精度会在一段时间后降低？我们怎么解决这个问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67d4e4-edb3-4f5d-97e1-bd2aa25bd4b2",
   "metadata": {},
   "source": [
    "增加迭代周期的数量可能会导致过拟合，从而导致测试精度下降。具体来说，当我们增加迭代周期的数量时，模型可能会开始学习到一些只能满足训练样本的非共性特征（这些更多是一种偶然性特征，不适用于测试样本），从而导致过拟合。为了解决这个问题，我们可以使用早停技术或正则化技术。早停技术是指在模型出现过拟合时（测试集表现开始下降）停止训练。正则化技术是指通过向损失函数添加惩罚项来限制模型参数的大小，从而减少过拟合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5cf25-fd0a-4842-b2d8-e3ad79c68115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

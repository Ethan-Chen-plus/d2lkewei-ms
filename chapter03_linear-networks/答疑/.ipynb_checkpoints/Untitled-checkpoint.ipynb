{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b418094-d968-40c7-a588-af2f8ec64068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kewei/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py:359: UserWarning: Model has no forward function\n",
      "  warnings.warn(\"Model has no forward function\")\n",
      "/home/kewei/miniconda3/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py:773: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to training  mode. The operators will be exported in training , as specified by the functional operator.\n",
      "  warnings.warn(\"ONNX export mode is set to \" + training_mode +\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Define your PyTorch model\n",
    "model = torchvision.models.resnet18()\n",
    "\n",
    "# Create some input data for your model\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Use your input data to run your model so that the dynamic graph can be captured\n",
    "traced_model = torch.jit.trace(model, example)\n",
    "\n",
    "# Export your traced model to ONNX format\n",
    "torch.onnx.export(traced_model, example, \"resnet18.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88a3230-3dfd-4dc4-9464-6c6d93fcc56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = w+x\n",
    "b = w+1\n",
    "y = a*b\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ad7f3b-7be6-4a95-a7aa-bb8eef16cdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = w+x\n",
    "b = w+1\n",
    "y = a*b\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a687d28d-9f76-44d9-9b2d-0b631e89be14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False False False\n",
      "tensor([5.]) tensor([2.]) None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kewei/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = w+x\n",
    "b = w+1\n",
    "y = a*b\n",
    "\n",
    "y.backward()\n",
    "print(w.is_leaf,x.is_leaf,a.is_leaf,b.is_leaf,y.is_leaf)\n",
    "print(w.grad,x.grad,a.grad,b.grad,y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128b3eac-2bea-49f2-8519-88315a4bddc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208/1158322773.py:13: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  for _ in range(torch.randint(1, 4, (1,)).item()):\n",
      "/home/kewei/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py:983: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1 / 2 (50.0%)\n",
      "Greatest absolute difference: 0.09249415993690491 at index (1, 0) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.595344628667434 at index (1, 0) (up to 1e-05 allowed)\n",
      "\n",
      "  _check_trace(\n"
     ]
    },
    {
     "ename": "TracingCheckError",
     "evalue": "Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.DynamicNet,\n\t\t        %x : Tensor):\n\t\t    %output_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t    %middle_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"middle_linear\"](%self.1)\n\t\t    %input_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"input_linear\"](%self.1)\n\t\t    %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%input_linear)\n\t\t    %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%input_linear)\n\t\t-   %16 : Tensor = aten::linear(%x, %weight.1, %bias.1), scope: __module.input_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t+   %20 : Tensor = aten::linear(%x, %weight.1, %bias.1), scope: __module.input_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t    %6 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t    %7 : NoneType = prim::Constant()\n\t\t-   %input.1 : Tensor = aten::clamp(%16, %6, %7) # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t?                                    ^^\n\t\t+   %input.1 : Tensor = aten::clamp(%20, %6, %7) # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t?                                    ^^\n\t\t    %bias.3 : Tensor = prim::GetAttr[name=\"bias\"](%middle_linear)\n\t\t    %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%middle_linear)\n\t\t-   %19 : Tensor = aten::linear(%input.1, %weight.3, %bias.3), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t+   %23 : Tensor = aten::linear(%input.1, %weight.3, %bias.3), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t    %10 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t    %11 : NoneType = prim::Constant()\n\t\t-   %input : Tensor = aten::clamp(%19, %10, %11) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t?                                  ^^\n\t\t+   %input.3 : Tensor = aten::clamp(%23, %10, %11) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t?         ++                         ^^\n\t\t+   %bias.5 : Tensor = prim::GetAttr[name=\"bias\"](%middle_linear)\n\t\t+   %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%middle_linear)\n\t\t+   %26 : Tensor = aten::linear(%input.3, %weight.5, %bias.5), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t+   %14 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t+   %15 : NoneType = prim::Constant()\n\t\t+   %input : Tensor = aten::clamp(%26, %14, %15) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%output_linear)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%output_linear)\n\t\t-   %22 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.output_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?     ^\n\t\t+   %29 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.output_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?     ^\n\t\t-   return (%22)\n\t\t?             ^\n\t\t+   return (%29)\n\t\t?             ^\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %output_linear : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t?                                                                      ^\n\t\t+ %output_linear : __torch__.torch.nn.modules.linear.___torch_mangle_257.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t?                                                                      ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracingCheckError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m example \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Use your input data to run your model so that the dynamic graph can be captured\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDynamicNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Export your traced model to ONNX format\u001b[39;00m\n\u001b[1;32m     25\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(traced_model, example, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamic_net.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/kewei/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py:741\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m ):\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    759\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[1;32m    760\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m         _module_class,\n\u001b[1;32m    768\u001b[0m     )\n",
      "File \u001b[0;32m/home/kewei/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py:983\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m                 _check_trace(\n\u001b[1;32m    973\u001b[0m                     check_inputs,\n\u001b[1;32m    974\u001b[0m                     func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    980\u001b[0m                     _module_class,\n\u001b[1;32m    981\u001b[0m                 )\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m                 \u001b[43m_check_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcheck_trace_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    994\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;241m=\u001b[39m old_module_map\n",
      "File \u001b[0;32m/home/kewei/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/kewei/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py:526\u001b[0m, in \u001b[0;36m_check_trace\u001b[0;34m(check_inputs, func, traced_func, check_tolerance, strict, force_outplace, is_trace_module, _module_class)\u001b[0m\n\u001b[1;32m    524\u001b[0m diag_info \u001b[38;5;241m=\u001b[39m graph_diagnostic_info()\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m diag_info):\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TracingCheckError(\u001b[38;5;241m*\u001b[39mdiag_info)\n",
      "\u001b[0;31mTracingCheckError\u001b[0m: Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.DynamicNet,\n\t\t        %x : Tensor):\n\t\t    %output_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t    %middle_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"middle_linear\"](%self.1)\n\t\t    %input_linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"input_linear\"](%self.1)\n\t\t    %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%input_linear)\n\t\t    %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%input_linear)\n\t\t-   %16 : Tensor = aten::linear(%x, %weight.1, %bias.1), scope: __module.input_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t+   %20 : Tensor = aten::linear(%x, %weight.1, %bias.1), scope: __module.input_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t    %6 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t    %7 : NoneType = prim::Constant()\n\t\t-   %input.1 : Tensor = aten::clamp(%16, %6, %7) # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t?                                    ^^\n\t\t+   %input.1 : Tensor = aten::clamp(%20, %6, %7) # /tmp/ipykernel_208/1158322773.py:12:0\n\t\t?                                    ^^\n\t\t    %bias.3 : Tensor = prim::GetAttr[name=\"bias\"](%middle_linear)\n\t\t    %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%middle_linear)\n\t\t-   %19 : Tensor = aten::linear(%input.1, %weight.3, %bias.3), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t+   %23 : Tensor = aten::linear(%input.1, %weight.3, %bias.3), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?    ^^\n\t\t    %10 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t    %11 : NoneType = prim::Constant()\n\t\t-   %input : Tensor = aten::clamp(%19, %10, %11) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t?                                  ^^\n\t\t+   %input.3 : Tensor = aten::clamp(%23, %10, %11) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t?         ++                         ^^\n\t\t+   %bias.5 : Tensor = prim::GetAttr[name=\"bias\"](%middle_linear)\n\t\t+   %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%middle_linear)\n\t\t+   %26 : Tensor = aten::linear(%input.3, %weight.5, %bias.5), scope: __module.middle_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t+   %14 : int = prim::Constant[value=0]() # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t+   %15 : NoneType = prim::Constant()\n\t\t+   %input : Tensor = aten::clamp(%26, %14, %15) # /tmp/ipykernel_208/1158322773.py:14:0\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%output_linear)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%output_linear)\n\t\t-   %22 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.output_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?     ^\n\t\t+   %29 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.output_linear # /home/kewei/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n\t\t?     ^\n\t\t-   return (%22)\n\t\t?             ^\n\t\t+   return (%29)\n\t\t?             ^\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %output_linear : __torch__.torch.nn.modules.linear.___torch_mangle_254.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t?                                                                      ^\n\t\t+ %output_linear : __torch__.torch.nn.modules.linear.___torch_mangle_257.Linear = prim::GetAttr[name=\"output_linear\"](%self.1)\n\t\t?                                                                      ^\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Define your PyTorch model using dynamic graph\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(torch.randint(1, 4, (1,)).item()):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# Create some input data for your model\n",
    "example = torch.randn(2, 2)\n",
    "\n",
    "# Use your input data to run your model so that the dynamic graph can be captured\n",
    "traced_model = torch.jit.trace(DynamicNet(2, 3, 1), example)\n",
    "\n",
    "# Export your traced model to ONNX format\n",
    "torch.onnx.export(traced_model, example, \"dynamic_net.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e04cc-0504-4a5d-8ec7-a2d49968f1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
